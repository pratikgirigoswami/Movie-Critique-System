{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9Du30kOgcmC"
   },
   "source": [
    "# ***INSTALLING PACKAGES***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3LgjPYSkyA-F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torch\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !conda install -c conda-forge python-telegram-bot\n",
    "# # # !conda install preprocessor\n",
    "# # # !conda install emoji\n",
    "# # # !conda install spacy\n",
    "# # # !conda install transformers # > 2.2.0\n",
    "# # # !conda install neuralcoref\n",
    "# # # !python -m spacy download en_core_web_md\n",
    "# # # !conda install bert-extractive-summarizer\n",
    "# !pip install bert-extractive-summarizer\n",
    "# !pip install python-telegram-bot\n",
    "# !pip install transformers==2.4.1\n",
    "# !curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n",
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bc4qm-IKgwEr"
   },
   "source": [
    "# ***IMPORTING LIBRARIES***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lTHoLY0PyS2D"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ym/c6zpr3ld14bbytgp5tp2n9l80000gn/T/ipykernel_94709/717899591.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msummarizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Lambton College/2nd Semester/Development/VirtualEnvironment/lib/python3.10/site-packages/summarizer/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_processors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummarizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerSummarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Summarizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TransformerSummarizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Lambton College/2nd Semester/Development/VirtualEnvironment/lib/python3.10/site-packages/summarizer/model_processors.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                           XLMTokenizer, XLNetModel, XLNetTokenizer)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert_parent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertParent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_features\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClusterFeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_handler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceHandler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive - Lambton College/2nd Semester/Development/VirtualEnvironment/lib/python3.10/site-packages/summarizer/bert_parent.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m from transformers import (AlbertModel, AlbertTokenizer, BertModel,\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from telegram.ext import *\n",
    "from datetime import datetime\n",
    "import time\n",
    "from summarizer import Summarizer\n",
    "import pytz\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re \n",
    "import string\n",
    "import preprocessor as p\n",
    "import json\n",
    "import os\n",
    "import emoji\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xuWeZEvg2zM"
   },
   "source": [
    "# ***MOVIE AND YEAR STRING CREATION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHsDK45Iq07q"
   },
   "outputs": [],
   "source": [
    "def tweetMovie(movieName, Date):\n",
    "  \n",
    "  movieName = movieName.replace(\" \",\"\")\n",
    "  S1 = \"#\" + movieName\n",
    "  S2 = \"#\" + movieName + \"review\"\n",
    "  S3 = \"#\" + movieName + Date\n",
    "  Date = Date + \"-01-01\"\n",
    "  S = S1 + \" OR \" + S2 + \" OR \" + S3\n",
    "\n",
    "  return S, Date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTdl_qEEhAII"
   },
   "source": [
    "# ***IMDB DATASET EXTRACTION AND CLEANING***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "WE0foNParRDq",
    "outputId": "3029ad10-b687-4908-bc67-6f141b9aff44"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/Drive/', force_remount = True)\n",
    "imdbData = pd.read_csv('/content/Drive/MyDrive/Term 2/CBD 3335/Final Project/IMDB Dataset.csv', header = 0)\n",
    "\n",
    "print(imdbData.shape)\n",
    "imdbData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "h8oioGmNved_",
    "outputId": "a06ae6ef-1a7f-455f-d6ce-c9eb3fb0eee8"
   },
   "outputs": [],
   "source": [
    "imdbData.sentiment.replace('positive',1,inplace=True)\n",
    "imdbData.sentiment.replace('negative',0,inplace=True)\n",
    "imdbData.rename(columns={'review': 'Review', 'sentiment': 'Sentiment'}, inplace=True)\n",
    "imdbData.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "7rkffOJvwNmS",
    "outputId": "ca466902-b218-4efe-dd38-8ff15118f0ee"
   },
   "outputs": [],
   "source": [
    "def Clean(Text):\n",
    "    \n",
    "    Cleaned = re.compile(r'<.*?>')\n",
    "    return re.sub(Cleaned, '', Text)\n",
    "\n",
    "imdbData.Review = imdbData.Review.apply(Clean)\n",
    "imdbData.Review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "Mvg13n4jwxbI",
    "outputId": "c81ab80c-ceab-4d55-db27-853cdefc71b9"
   },
   "outputs": [],
   "source": [
    "def isSpecial(Text):\n",
    "    \n",
    "    Rem = ''\n",
    "    for i in Text:\n",
    "        \n",
    "        if i.isalnum():\n",
    "            Rem = Rem + i\n",
    "        else:\n",
    "            Rem = Rem + ' '\n",
    "    \n",
    "    return Rem\n",
    "\n",
    "imdbData.Review = imdbData.Review.apply(isSpecial)\n",
    "imdbData.Review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "O3WBXqUZxESf",
    "outputId": "f67118f3-2053-4521-88d4-561591385b1f"
   },
   "outputs": [],
   "source": [
    "def toLower(Text):\n",
    "    \n",
    "    return Text.lower()\n",
    "\n",
    "imdbData.Review = imdbData.Review.apply(toLower)\n",
    "imdbData.Review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMuUDAqoxk-u",
    "outputId": "ef361a4a-120e-4811-aa03-63940e8ea684"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remStopwords(Text):\n",
    "    \n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    Words = word_tokenize(Text)\n",
    "    \n",
    "    return [W for W in Words if W not in stopWords]\n",
    "\n",
    "imdbData.Review = imdbData.Review.apply(remStopwords)\n",
    "imdbData.Review[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "Un42YJBmyz7x",
    "outputId": "1a6c2780-93a9-41f3-d104-11070b1c069b"
   },
   "outputs": [],
   "source": [
    "def stemText(Text):\n",
    "    \n",
    "    SS = SnowballStemmer('english')\n",
    "    \n",
    "    return \" \".join([SS.stem(W) for W in Text])\n",
    "\n",
    "imdbData.Review = imdbData.Review.apply(stemText)\n",
    "imdbData.Review[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_JWOwQZhPzt"
   },
   "source": [
    "# ***TRAIN TEST SPLIT***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvzVG7nQzH7I",
    "outputId": "48a48b66-dc56-42df-eb27-594a95297724"
   },
   "outputs": [],
   "source": [
    "X = np.array(imdbData.iloc[:,0].values)\n",
    "Y = np.array(imdbData.Sentiment.values)\n",
    "CV = CountVectorizer(max_features = 1000)\n",
    "X = CV.fit_transform(imdbData.Review).toarray()\n",
    "\n",
    "print(\"X Shape = \", X.shape)\n",
    "print(\"y Shape = \", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zLxzM5Ik0AQ6",
    "outputId": "57ee83ef-3678-44c1-c250-2db3c99f7ee3"
   },
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X, Y, test_size=0.2, random_state=9)\n",
    "\n",
    "print(\"Train Shapes : X = {}, Y = {}\".format(trainX.shape, trainY.shape))\n",
    "print(\"Test Shapes : X = {}, Y = {}\".format(testX.shape, testY.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVyBHN2jhUWH"
   },
   "source": [
    "# ***MODEL SELECTION***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJ0Og8kd0Wxr",
    "outputId": "bd718b8f-14bd-4ed3-b9f3-2bbf8b7f00d4"
   },
   "outputs": [],
   "source": [
    "GNB, MNB, BNB = GaussianNB(), MultinomialNB(alpha=1.0,fit_prior=True), BernoulliNB(alpha=1.0,fit_prior=True)\n",
    "GNB.fit(trainX, trainY)\n",
    "MNB.fit(trainX, trainY)\n",
    "BNB.fit(trainX, trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "piwpYVb11h7O",
    "outputId": "46cbf72e-85cb-4302-ffdc-d7d5c0ab232a"
   },
   "outputs": [],
   "source": [
    "YPG = GNB.predict(testX)\n",
    "YPM = MNB.predict(testX)\n",
    "YPB = BNB.predict(testX)\n",
    "\n",
    "print(\"Gaussian = \", accuracy_score(testY, YPG))\n",
    "print(\"Multinomial = \", accuracy_score(testY, YPM))\n",
    "print(\"Bernoulli = \", accuracy_score(testY, YPB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oi3b-_XxhX1b"
   },
   "source": [
    "# ***TWITTER API VALUES***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_9SW6pO276G"
   },
   "outputs": [],
   "source": [
    "accessKey = '1076801107-YnSnzPG8QVLCHTpAIzH0B0iN5y4Ktg3ooWBtC32'\n",
    "accessSecret = 'tu6pZ59yae4PbBWl7VamRjUgLzaV3Dl1hWpjS96tvCCjk'\n",
    "consumerKey = 'yUv9lg35fpu56VJPPgxx54KQI'\n",
    "consumerSecret ='hGgAALXQrIiB86gSmFxHqj3Qm3RRHEmPHh1sq5UW702tR32k1D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XD0JPMir3FSo"
   },
   "outputs": [],
   "source": [
    "Auth = tw.OAuthHandler(consumerKey, consumerSecret)\n",
    "Auth.set_access_token(accessKey, accessSecret)\n",
    "API = tw.API(Auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cr6-SuWDhbuJ"
   },
   "source": [
    "# ***TELEGRAM BOT***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsSAcNdNpwSS"
   },
   "outputs": [],
   "source": [
    "MOVIE, YEAR = range(0,2)\n",
    "\n",
    "def Start(Update, CallbackContext):\n",
    "\n",
    "    Update.message.reply_text('Hello! This Is Critique - Automated Movie Review System!\\n\\nType The Name Of The Movie:')\n",
    "\n",
    "    return MOVIE\n",
    "\n",
    "def Movie(Update, CallbackContext):\n",
    "    \n",
    "    global movieName \n",
    "    movieName = Update.message.text\n",
    "    Update.message.reply_text('\\nType The Year Of The Movie:')\n",
    "    \n",
    "    return YEAR\n",
    "\n",
    "def Year(Update, CallbackContext):\n",
    "\n",
    "    movieYear = Update.message.text\n",
    "    searchWords, dateSince = tweetMovie(movieName, movieYear)\n",
    "    newSearch = searchWords + \" -filter:links\"\n",
    "    Update.message.reply_text(\"\\n\\nBot At Work, Please Wait...\\n\\n\")\n",
    "\n",
    "    #TWITTER DATA COLLECTION\n",
    "    Tweets = tw.Cursor(API.search, \n",
    "                           q=newSearch,\n",
    "                           lang=\"en\",\n",
    "                           since=dateSince).items(5000)\n",
    "\n",
    "    usersLocs = [[Tweet.user.screen_name, Tweet.user.location, Tweet.text] for Tweet in Tweets]\n",
    "\n",
    "    tweetText = pd.DataFrame(data=usersLocs, columns=['User', 'Location', 'Tweet'])\n",
    "\n",
    "    tweetList = list(tweetText.Tweet)\n",
    "\n",
    "    cleanTweets = []\n",
    "\n",
    "    #TWITTER DATA CLEANING\n",
    "    for Tweet in tweetList:\n",
    "\n",
    "      Tweet = re.sub(\"@[A-Za-z0-9]+\",\"\", Tweet)\n",
    "      Tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", Tweet)\n",
    "      Tweet = \" \".join(Tweet.split())\n",
    "      Tweet = ''.join(c for c in Tweet if c not in emoji.UNICODE_EMOJI['en'])\n",
    "      Tweet = Tweet.replace(\"#\", \"\").replace(\"_\", \" \")\n",
    "      Tweet = \" \".join(w for w in nltk.wordpunct_tokenize(Tweet) \\\n",
    "          if w.lower() in words or not w.isalpha())\n",
    "      cleanTweets.append(Tweet)\n",
    "    \n",
    "    cleanTweets2=[]\n",
    "\n",
    "    for Tweet in cleanTweets:\n",
    "      if(len(Tweet) > 5):\n",
    "        cleanTweets2.append(Tweet)\n",
    "    \n",
    "    #SUMMARIZATION\n",
    "    movieSummary= \" \".join(Tweet for Tweet in cleanTweets2)\n",
    "\n",
    "    Model = Summarizer()\n",
    "    Summary = Model(movieSummary)\n",
    "    \n",
    "    pickle.dump(BNB,open('Model.pkl','wb'))\n",
    "    Rev = Summary\n",
    "    F1 = Clean(Rev)\n",
    "    F2 = isSpecial(F1)\n",
    "    F3 = toLower(F2)\n",
    "    F4 = remStopwords(F3)\n",
    "    F5 = stemText(F4)\n",
    "\n",
    "    Bow, Words = [], word_tokenize(F5)\n",
    "    \n",
    "    for Word in Words:\n",
    "        Bow.append(Words.count(Word))\n",
    "    \n",
    "    wordDict = CV.vocabulary_\n",
    "    pickle.dump(wordDict,open('Bow.pkl','wb'))\n",
    "\n",
    "    Inp = []\n",
    "    for i in wordDict:\n",
    "        Inp.append(F5.count(i[0]))\n",
    "    \n",
    "    #CLASSIFICATION USING BNB\n",
    "    yPred = BNB.predict(np.array(Inp).reshape(1,1000))\n",
    "\n",
    "    #OUTPUTING THE RESULT USING TELEGRAM BOT\n",
    "    print(\"Summarized Review :\\n\", Summary)\n",
    "    Update.message.reply_text(\"Summarized Review :\\n \")\n",
    "    Update.message.reply_text(Summary)\n",
    "\n",
    "    if (yPred==1):\n",
    "      print(\"The Movie: \",movieName ,\" Has Positive Sentiment Reviews\")\n",
    "      Update.message.reply_text(\"The Movie: \" + movieName + \" Has Positive Sentiment Reviews.\")\n",
    "    else:\n",
    "      print(\"The Movie: \",movieName ,\" Has Negative Sentiment Reviews\")\n",
    "      Update.message.reply_text(\"The Movie: \" + movieName + \" Has Negative Sentiment Reviews.\")\n",
    "    \n",
    "\n",
    "    Update.message.reply_text(\"\\n\\nPress /start To Start A New Review Process Or Goodbye, Fow Now <3.\")\n",
    "\n",
    "\n",
    "    return ConversationHandler.END\n",
    "\n",
    "def Bye(Update, CallbackContext):\n",
    "\n",
    "    Update.message.reply_text('Bye! I hope we can talk again some day.', reply_markup=ReplyKeyboardRemove())\n",
    "\n",
    "    return ConversationHandler.END\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(\"Bot Has Started.\")\n",
    "  \n",
    "    updaterVar = Updater(\"2102220766:AAEbLz-RNQjm9SDmilcZhExDgglEnSS-Xe0\", use_context=True)\n",
    "    Dispatcher = updaterVar.dispatcher\n",
    "\n",
    "    convHandler = ConversationHandler(\n",
    "        \n",
    "                                        entry_points=[CommandHandler('start', Start)],\n",
    "                                        states={\n",
    "                                            \n",
    "                                                  MOVIE: [MessageHandler(Filters.text, Movie)],\n",
    "                                                  YEAR:  [MessageHandler(Filters.text &  ~Filters.command, Year)]\n",
    "                                              \n",
    "                                               },\n",
    "\n",
    "                                        fallbacks=[CommandHandler('Bye', Bye)],\n",
    "                                    )\n",
    "\n",
    "    Dispatcher.add_handler(convHandler)\n",
    "    \n",
    "    Dispatcher.add_handler(CommandHandler(\"Start\", Start))\n",
    "\n",
    "    updaterVar.start_polling()\n",
    "    updaterVar.idle()\n",
    "\n",
    "main()\n",
    "print(\"Bot Has Stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CBD 3335 Final Project Group EWP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "VirtualEnvironment",
   "language": "python",
   "name": "virtualenvironment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
